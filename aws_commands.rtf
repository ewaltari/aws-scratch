AWS commands:

### CREATE INSTANCE
## the specops instance image has aws, git BUT NOT DOCKER

aegea launch waltari-immcantation --ami-tags Name=czbiohub-specops --iam-role S3fromEC2 -t m4.4xlarge
aegea ssh ubuntu@waltari-immcantation

## To stop:
aegea stop waltari-immcantation
## To restart
aegea start waltari-immcantation

## FYI: if after restarting your Mac, YOU MAY NOT BE ABLE TO USE aegea start (name)
  ## if this happens, use ssh: point to .pem file and use ubuntu@latest.address
  ## also note every time you stop and restart, the long address name changes!
ssh -i .ssh/aegea.launch.eric.waltari.waltari-mbp.pem ubuntu@XXX.us-west-2.compute.amazonaws.com
  ## to get full name - go to 
https://us-west-2.console.aws.amazon.com/ec2/v2/home?region=us-west-2#Instances:sort=keyName
  ## Then click on immcantation instance tab, ‘connect’, and an instruction box will pop-up with the address to use

————
### INSTALLING & STARTING DOCKER
## install with these commands

## install with these commands (http://ray.readthedocs.io/en/latest/install-on-docker.html)
## The instructions below show in detail how to prepare an Amazon EC2 instance running Ubuntu 16.04 for use with Docker. 
## initialize the package repository and apply system updates:
sudo apt-get update
sudo apt-get -y dist-upgrade
## Install Docker and start the service:
sudo apt-get install -y docker.io
sudo service docker start
## add the ubuntu user to the docker group to allow running Docker commands without sudo:
sudo usermod -a -G docker ubuntu

## Log out and log back in again…
## Confirm that docker is running:
docker images

## NEED TO CHANGE DIRECTORY TO WHERE DOCKER WORKS:
https://github.com/IronicBadger/til/blob/master/docker/change-docker-root.md
## then restart docker

————
### TRANSFER FILES FROM S3
## first need to configure aws:
aws configure
## basic: create directories you need
mkdir mnt/data/presto

## basic: type all aws commands from root directory before running
## transferring base scripts, some initial .yaml files and unzipped fastqs from first MiSeq runs (all in my s3 bucket):
aws s3 cp --recursive s3://eric.waltari-bucket/BX_miseq/presto/ /mnt/data/presto


## example to transfer one MiSeq run to aegea instance
aws s3 cp --recursive s3://czbiohub-seqbot/fastqs/180128_M05295_0078_000000000-BJNBT/rawdata/presto/ /mnt/data/presto

## basic: unzip fastq.gz files before running PRESTO
gunzip *fastq.gz
## basic: make sure to set chmod for scripts
chmod a+x *.sh

————
### DOCKER COMMANDS
docker pull kleinstein/immcantation:1.10.0 
## MARCH 1.7.0, APRIL 1.8.0, LATE APRIL 1.9.0, Late May 1.10.0 (http://immcantation.readthedocs.io/en/latest/docker/news.html)

## basic: first go to root directory before running docker

## to get docker shell to see files on Aegea
docker run -it -v /mnt/data/presto:/data:z kleinstein/immcantation:1.10.0 bash
## or to get to docker shell to see files locally
docker run -it -v $HOME/testing/2018mar_miseq_full:/data:z kleinstein/immcantation:1.10.0 bash
## use to make sure that all files are where they should be (check /data and /usr/local/share folders are also there)

## also to check Docker runs, and stop if necessary
docker ps                 # get the id of the running container
docker stop <containerid> # kill it (gracefully)

————
## FIRST STEP OF IMMCANTATION: PRESTO
  ## NOTE SETTINGS IN presto-abseq.sh
  ## PRIMER THRESHOLD 0.3
  ## IN PARTICULAR THE # OF MINIMUM BARCODES (LATEST SETTING IS 5)

## in mid-March first full run of BX MiSeq dataset
## in April changed primer names so can combine them (also added a line in presto-abseq.sh commands to do the combine step)
            ## the line I added: ParseHeaders.py copy -s reads.fastq -f PRCONS -k PRIMER --act first
## then in mid-May noticed another primer sequence error, final versions of primers.fasta files are made between 4.27 and 5.23


## DOCKER COMMAND TO RUN PRESTO IN AEGEA
docker run -v /mnt/data/presto:/data:z kleinstein/immcantation:1.10.0 /data/presto-abseq.sh -1 /data/Undetermined_S0_L001_R2_001.fastq -2 /data/Undetermined_S0_L001_R1_001.fastq -j /data/primers_R2c.fasta -v data/primers_R1.fasta -r /usr/local/share/igblast/fasta/imgt_human_ig_v.fasta -y /data/apr18_full.yaml -n BXjan5 -o /data/BXjan_filter5 -p 16 | sudo tee run_presto.out


## when each run is done (preferably before final step of zipping most intermediate files), transfer files back to s3
  ## three fastq outputs, and the .pdf report
  ## examples:
aws s3 cp /mnt/data/presto/BXapr_filter5/BXapr5-final_collapse-unique.fastq s3://eric.waltari-bucket/BX_miseq/presto/output/BXapr5-final_collapse-unique.fastq
aws s3 cp /mnt/data/presto/BXapr_filter5/BXapr5-final_collapse-unique_atleast-5.fastq s3://eric.waltari-bucket/BX_miseq/presto/output/BXapr5-final_collapse-unique_atleast-5.fastq
aws s3 cp /mnt/data/presto/BXapr_filter5/BXapr5-final_total.fastq s3://eric.waltari-bucket/BX_miseq/presto/output/BXapr5-final_total.fastq
aws s3 cp /mnt/data/presto/BXapr_filter5/report/MiSeq_apr_BX_full5_04272018.pdf s3://eric.waltari-bucket/BX_miseq/presto/output/MiSeq_apr_BX_full5_04272018.pdf


## note when a run is done, the remaining fastqs are not gzipped - also in /logs folder, .tab files are not zipped either
gzip *.fastq
gzip *.tab

————
## SECOND STEPS OF IMMCANTATION: CHANGEO
## STEP 2A: IGBLAST
  ## this script includes creating germlines and parsing only functional (example scripts break this into 3 parts)
## running locally:
  ## first put BX-final_collapse-unique_atleast-5.fastq into your folder
  ## first run bash in container to make sure  /usr/local/share/germlines/imgt/human/vdj has IMGT-gapped reference germlines!!
  ## 750k reads takes 3.5 hours
docker run -v $HOME/immcantation_pipeline/BX_min5/changeo:/data:z kleinstein/immcantation:1.10.0 /data/changeoV3.7_IgBLASTV1.7.sh -s /data/BXapr5-final_collapse-unique_atleast-5.fastq -n BXapr -o /data -p 4 | tee run_igblast.out

## STEP 2B: INFERENCE OF V SEGMENT GENOTYPES (TIGGER)
     ## FULL DATASET RETURNED ERRORS - NOW USING LOCAL R SCRIPT
TiggerV2.10_Genotype.R

## STEP 2C: CLONAL ASSIGNMENT THRESHOLDING
    ## FULL DATASET RETURNED ERRORS - NOW USING LOCAL R SCRIPT
    ## R script of full dataset = 1 hour for db step, ~2 more hours for threshold step
ShazamV1.10Distance.R

### STEP2D: CLONAL ASSIGNMENT
  # FIRST GET THRESHOLD (then add to -x below) FROM STEP 2C
docker run -v $HOME/immcantation_pipeline/BX_min5/changeo:/data:z kleinstein/immcantation:1.10.0 /data/ChangeoV3.7_Clone.sh -d /data/BX_genotyped5t.tab -x 0.0692 -n BX -o /data -p 4 | tee run_clone.out


————
## FOR MAKING NETWORKS (IN CHANGEO) THAT INCLUDE SEQUENCES FOUND VIA SORTING, MAKE FASTA FILES OF SUCH SEQUENCES THEN RUN:
docker run -v $HOME/testing/2018mar_miseq_full:/data:z kleinstein/immcantation:1.10.0 /data/changeoV3.7_IgBLASTV1.7.sh -s /data/moreINFandAD2.fasta -n moreInfAandAD2 -o /data -p 4 | tee run_igblast.out
docker run -v $HOME/testing/2018mar_miseq_full:/data:z kleinstein/immcantation:1.10.0 shazam-threshold -d /data/moreInfAandAD2_parse-select.tab -n moreInfAandAD2 -o /data -p 4 | tee run_threshold.out
docker run -v $HOME/testing/2018mar_miseq_full:/data:z kleinstein/immcantation:1.10.0 /data/ChangeoV3.7_Clone.sh -d /data/moreInfAandAD2_parse-select.tab -x 0.0396 -n moreInfAandAD2 -o /data -p 4 | tee run_clone.out



————
##############################################################
############# SINGLE CELL TRANSCRIPTOME COMMANDS #############
##############################################################

### 5/4 learning from James that a very easy workflow exists:
https://github.com/czbiohub/utilities

## set up utilities-env
conda create --name utilities-env python pip

#run
  ## our run is here 180416_NB501961_0098_AHLNCFBGX5
  ## example aws_star mus 10 YYMMDD_EXP_ID > my_star_jobs.sh
### commands
aws_star homo 24 180416_NB501961_0098_AHLNCFBGX5 > my_star_jobs.sh
cat my_star_jobs.sh
source my_star_jobs.sh

## then aligning
  ## gene_cell_table fastqs/YYMMDD_EXP_ID YYMMDD_EXP_ID.csv --dryrun
gene_cell_table fastqs/180416_NB501961_0098_AHLNCFBGX5 180416_NB501961_0098_AHLNCFBGX5.csv


